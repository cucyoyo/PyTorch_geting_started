{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[  1.00000000e+00   1.10000000e-05              nan              nan\n",
      "              nan   2.00000000e+00   0.00000000e+00   4.96000000e+02\n",
      "   0.00000000e+00   9.09090902e+04   2.54000000e+02   0.00000000e+00\n",
      "   1.80363632e+08   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   1.10000000e-02   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   2.48000000e+02\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   2.00000000e+00\n",
      "   2.00000000e+00   1.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "   2.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   1.00000000e+00   2.00000000e+00   0.00000000e+00              nan\n",
      "   0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data as Data\n",
    "# 读取csv数据到numpy数组中\n",
    "from numpy import genfromtxt\n",
    "\n",
    "import types\n",
    "\n",
    "torch.manual_seed(1)    # reproducible\n",
    "\n",
    "# Hyper Parameters\n",
    "EPOCH = 1           # 训练整批数据多少次, 为了节约时间, 我们只训练一次\n",
    "BATCH_SIZE = 50\n",
    "LR = 0.001          # 学习率\n",
    "\n",
    "\n",
    "# 读取csv数据到numpy数组中\n",
    "train_data = genfromtxt('data/NB15-300.csv', delimiter=',')\n",
    "# 第一行是名称，去掉\n",
    "train_data = train_data[1:]\n",
    "print(type(train_data))\n",
    "print(train_data[0])\n",
    "\n",
    "# 批训练 50samples, 1 channel, 28x28 (50, 1, 28, 28)\n",
    "train_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# # 提取test_data\n",
    "test_data = genfromtxt('data/NB15-300.csv', delimiter=',')\n",
    "test_data = train_data[1:]\n",
    "\n",
    "# test_features = Variable(torch.unsqueeze(test_data[,:1], dim=1), volatile=True).type(torch.FloatTensor)/28.   # shape from (2000, 28, 28) to (2000, 1, 28, 28), value in range(0,1)\n",
    "# test_lables = test_data.test_labels[:2000]\n",
    "\n",
    "# #### 创建CNN网络\n",
    "# class CNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(CNN, self).__init__()\n",
    "#         self.conv1 = nn.Sequential(  # input shape (1, 28, 28)\n",
    "#             # 卷积层 过滤器 filter的高度为了提取属性\n",
    "#             nn.Conv2d(\n",
    "#                 in_channels=1,      # input height；输入高度\n",
    "#                 out_channels=16,    # n_filters：输出高度，filter个数，16个filter同时扫描，提取了16个特征，作为下一次的输入\n",
    "#                 kernel_size=5,      # filter size：filter的长和宽是5\n",
    "#                 stride=1,           # filter movement/step：filter扫描的跳度\n",
    "#                 padding=2,      # 如果想要 con2d 出来的图片长宽没有变化, padding=(kernel_size-1)/2 当 stride=1\n",
    "#             ),      # output shape (16, 28, 28)\n",
    "#             # 激励函数\n",
    "#             nn.ReLU(),    # activation\n",
    "#             # 池化层\n",
    "#             nn.MaxPool2d(kernel_size=2),    # 在 2x2 空间里向下采样, output shape (16, 14, 14)\n",
    "#         )                                   # maxpool:2*2里面选取最大的；avaragepool：选取2*2里的平均值\n",
    "#         self.conv2 = nn.Sequential(  # input shape (16, 14, 14)\n",
    "#             nn.Conv2d(16, 32, 5, 1, 2),  # output shape (32, 14, 14)\n",
    "#             nn.ReLU(),  # activation\n",
    "#             nn.MaxPool2d(2),  # output shape (32, 7, 7)\n",
    "#         )\n",
    "#         self.out = nn.Linear(32 * 7 * 7, 10)   # fully connected layer, output 10 classes\n",
    "\n",
    "#     # 将上述3位数据展平为二维数据\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.conv2(x)\n",
    "#         # -1负责把所有维度放到一起\n",
    "#         x = x.view(x.size(0), -1)   # 展平多维的卷积图成 (batch_size, 32 * 7 * 7)\n",
    "#         output = self.out(x)\n",
    "#         return output, x\n",
    "\n",
    "# cnn = CNN()\n",
    "# print(cnn)  # net architecture\n",
    "# # print(x)\n",
    "\n",
    "# #### 训练\n",
    "# optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)   # optimize all cnn parameters\n",
    "# loss_func = nn.CrossEntropyLoss()   # the target label is not one-hotted\n",
    "\n",
    "# # training and testing\n",
    "# for epoch in range(EPOCH):\n",
    "#     for step, (x, y) in enumerate(train_loader):   # gives batch data, normalize x when iterate train_loader\n",
    "#         b_x = Variable(x)   # batch x\n",
    "#         b_y = Variable(y)   # batch y\n",
    "\n",
    "#         output = cnn(b_x)[0]               # cnn output\n",
    "#         loss = loss_func(output, b_y)   # cross entropy loss\n",
    "#         optimizer.zero_grad()           # clear gradients for this training step\n",
    "#         loss.backward()                 # backpropagation, compute gradients\n",
    "#         optimizer.step()                # apply gradients\n",
    "\n",
    "#         if step % 50 == 0:\n",
    "#             test_output, last_layer = cnn(test_x)\n",
    "#             pred_y = torch.max(test_output, 1)[1].data.squeeze()\n",
    "#             accuracy = sum(pred_y == test_y) / float(test_y.size(0))\n",
    "#             print('Epoch: ', epoch, '| train loss: %.4f' % loss.data[0], '| test accuracy: %.2f' % accuracy)\n",
    "           \n",
    "# # print 10 predictions from test data\n",
    "# test_output, _ = cnn(test_x[:10])\n",
    "# pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "# print(pred_y, 'prediction number')\n",
    "# print(test_y[:10].numpy(), 'real number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '_csv.reader'>\n",
      "<_csv.reader object at 0x7fe5d243ccf8>\n",
      "39.48\n",
      "71.38\n",
      "62.58\n",
      "98.31\n",
      "53.08\n",
      "78.29\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import types\n",
    "with open('demo.csv') as f:\n",
    "    \n",
    "    f_csv = csv.reader(f)\n",
    "    print(type(f_csv))\n",
    "    \n",
    "    headers = next(f_csv)\n",
    "    print(f_csv)\n",
    "    for row in f_csv:\n",
    "        print(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '0.121478', 'tcp', '-', 'FIN', '6', '4', '258', '172', '74.08749', '252', '254', '14158.94238', '8495.365234', '0', '0', '24.2956', '8.375', '30.177547', '11.830604', '255', '621772692', '2202533631', '255', '0', '0', '0', '43', '43', '0', '0', '1', '0', '1', '1', '1', '1', '0', '0', '0', '1', '1', '0', 'Normal', '0']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "with open('UNSW_NB15_testing-set.csv') as f:\n",
    "    f_csv = csv.reader(f)\n",
    "    headers = next(f_csv)\n",
    "    i = 0\n",
    "    for row in f_csv:\n",
    "        print(row)\n",
    "        i = i + 1\n",
    "        if i == 1:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1.1e-05 'udp' ..., 1 2 0]\n",
      " [2 8e-06 'udp' ..., 1 2 0]\n",
      " [3 5e-06 'udp' ..., 1 3 0]\n",
      " ..., \n",
      " [297 0.43261700000000003 'tcp' ..., 1 1 0]\n",
      " [298 0.249214 'tcp' ..., 2 2 0]\n",
      " [299 0.647249 'tcp' ..., 1 1 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:19: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data as Data\n",
    "import pandas as pd\n",
    "\n",
    "import types\n",
    "\n",
    "torch.manual_seed(1)    # reproducible\n",
    "\n",
    "# Hyper Parameters\n",
    "EPOCH = 1           # 训练整批数据多少次, 为了节约时间, 我们只训练一次\n",
    "BATCH_SIZE = 50\n",
    "LR = 0.001          # 学习率\n",
    "\n",
    "train_data = pd.read_csv('data/NB15-300.csv')\n",
    "train_features = train_data.ix[:,:-2].values\n",
    "train_lables1 = train_data.ix[:,-1].values\n",
    "train_lables2 = train_data.ix[:,-2].values\n",
    "print(train_features)\n",
    "\n",
    "test_data = pd.read_csv('data/NB15-test-300.csv')\n",
    "test_features = test_data.ix[:,:-2].values\n",
    "test_lables1 = test_data.ix[:,-1].values\n",
    "test_lables2 = test_data.ix[:,-2].values\n",
    "print(test_features)\n",
    "\n",
    "\n",
    "#### 创建CNN网络\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(  # input shape (1, 28, 28)\n",
    "            # 卷积层 过滤器 filter的高度为了提取属性\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,      # input height；输入高度\n",
    "                out_channels=16,    # n_filters：输出高度，filter个数，16个filter同时扫描，提取了16个特征，作为下一次的输入\n",
    "                kernel_size=5,      # filter size：filter的长和宽是5\n",
    "                stride=1,           # filter movement/step：filter扫描的跳度\n",
    "                padding=2,      # 如果想要 con2d 出来的图片长宽没有变化, padding=(kernel_size-1)/2 当 stride=1\n",
    "            ),      # output shape (16, 28, 28)\n",
    "            # 激励函数\n",
    "            nn.ReLU(),    # activation\n",
    "            # 池化层\n",
    "            nn.MaxPool2d(kernel_size=2),    # 在 2x2 空间里向下采样, output shape (16, 14, 14)\n",
    "        )                                   # maxpool:2*2里面选取最大的；avaragepool：选取2*2里的平均值\n",
    "        self.conv2 = nn.Sequential(  # input shape (16, 14, 14)\n",
    "            nn.Conv2d(16, 32, 5, 1, 2),  # output shape (32, 14, 14)\n",
    "            nn.ReLU(),  # activation\n",
    "            nn.MaxPool2d(2),  # output shape (32, 7, 7)\n",
    "        )\n",
    "        self.out = nn.Linear(32 * 7 * 7, 10)   # fully connected layer, output 10 classes\n",
    "\n",
    "    # 将上述3位数据展平为二维数据\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        # -1负责把所有维度放到一起\n",
    "        x = x.view(x.size(0), -1)   # 展平多维的卷积图成 (batch_size, 32 * 7 * 7)\n",
    "        output = self.out(x)\n",
    "        return output, x\n",
    "\n",
    "cnn = CNN()\n",
    "print(cnn)  # net architecture\n",
    "# print(x)\n",
    "\n",
    "#### 训练\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)   # optimize all cnn parameters\n",
    "loss_func = nn.CrossEntropyLoss()   # the target label is not one-hotted\n",
    "\n",
    "# training and testing\n",
    "for epoch in range(EPOCH):\n",
    "    for step in enumerate(train_features):   # gives batch data, normalize x when iterate train_loader\n",
    "        b_x = Variable(torch.FloatTensor(train_features))   # batch x\n",
    "        b_y = Variable(y)   # batch y\n",
    "\n",
    "        output = cnn(b_x)[0]               # cnn output\n",
    "        loss = loss_func(output, b_y)   # cross entropy loss\n",
    "        optimizer.zero_grad()           # clear gradients for this training step\n",
    "        loss.backward()                 # backpropagation, compute gradients\n",
    "        optimizer.step()                # apply gradients\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            test_output, last_layer = cnn(test_x)\n",
    "            pred_y = torch.max(test_output, 1)[1].data.squeeze()\n",
    "            accuracy = sum(pred_y == test_y) / float(test_y.size(0))\n",
    "            print('Epoch: ', epoch, '| train loss: %.4f' % loss.data[0], '| test accuracy: %.2f' % accuracy)\n",
    "           \n",
    "# print 10 predictions from test data\n",
    "test_output, _ = cnn(test_x[:10])\n",
    "pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "print(pred_y, 'prediction number')\n",
    "print(test_y[:10].numpy(), 'real number')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
